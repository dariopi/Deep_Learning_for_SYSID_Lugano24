{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b15d6fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise on Back-propagation (only for the students who want to practice with backpropagation)\n",
    "\n",
    "### Course on Deep Learning for System Identification\n",
    "### Authors: Dario Piga, Marco Forgione\n",
    "### Lugano, March 7th, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213aba98",
   "metadata": {},
   "source": [
    "Let us consider the 4-layer network  reported in the figure below:\n",
    "\n",
    "\n",
    "![4-layer network](figures/forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c7f7e",
   "metadata": {},
   "source": [
    "Consider the following conditions:\n",
    "\n",
    "- $x \\in R^2; y \\in R$. Both $x$ and $y$ are given and equal to: $x = [1, \\ 2]^T$ and $y = 1$. \n",
    "- Layer 1 is a Linear Layer, mapping $z^1=x$ into $z^2 \\in R^3$ as follows: $z^2 = W_1 z^1 + b_1$, where $W_1$ is matrix of size 3x2, and $b_1$ is a vector of size 3x1\n",
    "- Layer 2 is a nonlinear layer made of sigmoid activation functions, mapping elementwise each element of $z^2$ into $z^3$ as follows:\n",
    "$z^3_j = \\sigma(z^2_j)$, where $\\sigma$ is a sigmoid function defined as $$\\sigma(z)=\\frac{e^{z}}{1+e^z}$$\n",
    "- Layer 3 is a Linear Layer, mapping $z^3$ into $z^4 \\in R$ as follows: $z^4 = W_3 z^3 + b_3$, where $W_3$ is matrix of size 1x3 and $b_3$ is scalar\n",
    "- Layer 4 simply takes $z^4$ as input and constructs a quadratic scalar loss $\\ell$\n",
    "\n",
    "The matrices $W_1, b_1$ and $W_3, b_3$ are parameters of the network. \n",
    "\n",
    "Exercise: Apply the backpropagation algorithm to compute the gradient of the loss (namely, $z_5$) w.r.t. the parameters $W_1$, $b_1$, $W_3$ and $b_3$, evaluated at the following values of the matrices $W_1$, $b_1$, $W_3$ and $b_3$:\n",
    "\n",
    "$$\n",
    "W_1 = \\left[ \\begin{array}{ll}\n",
    "0 & 1 \\\\\n",
    "1 & -2 \\\\\n",
    "-2 & 1\n",
    "\\end{array} \\right], \\ \\ \\ b_1 = \\left[ \\begin{array}{l}\n",
    "1  \\\\\n",
    "1  \\\\\n",
    "-1\n",
    "\\end{array} \\right],  \\ \\ \\ W_3 = \\left[ \\begin{array}{lll}\n",
    "1 & 1 & 1\n",
    "\\end{array} \\right], \\ \\ \\ b_3 = 1.\n",
    "$$\n",
    "\n",
    "To compact the notation, we stack $W_1$ and $b_1$ in the parameter $\\theta_1$. Similarly, we stack $W_3$ and $b_3$ in the parameter $\\theta_3$.\n",
    "\n",
    "\n",
    "**For your practice, we invite you to solve the exercise with \"pen and paper\", with the aid of a calculator (or Python itself) just for algebraic computations, such as matrix multiplication, evaluation of the output of a sigmoid funcation, etc.**\n",
    "\n",
    "**At the end of the exercise, you can compare your solution with the one computed through the backward function in PyTorch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3c677",
   "metadata": {},
   "source": [
    "**Hint 1:**\n",
    "\n",
    "Apply the **forward pass**  according to the diagram in the cell above\n",
    "\n",
    "Apply the **backward pass** according to the following diagram: \n",
    "\n",
    "![4-layer network](figures/backward.png)\n",
    " \n",
    "where each element of $\\delta^L$ is computed recursively according to the following formula seen in the lesson:\n",
    "\n",
    " \\begin{align*}\n",
    " \\delta_i^L =     \\delta^{L+1}   \\frac{\\partial z^{L+1}}{\\partial z_i^{L}}\n",
    " \\end{align*}\n",
    " \n",
    " Thus, you need to compute $\\frac{\\partial z^{L+1}}{\\partial z_i^{L}}$, and compute $\\delta^L$ recursively from $\\delta^{L+1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b77cc",
   "metadata": {},
   "source": [
    "**Hint 2:**\n",
    "\n",
    "Once the derivatives $\\delta^L$  (with $L=1,2,3,4$) are computed, the partial derivatives $\\frac{\\partial \\ell}{\\partial \\theta_1} \\ \\ \\text{ and   } \\ \\  \\frac{\\partial \\ell}{\\partial \\theta_3}$  can be computed according to the general formula seen at the lesson:\n",
    "\n",
    "$$  \\frac{\\partial \\ell}{\\partial \\theta_L} = \\sum_i  \\delta_i^{L+1}  \\frac{\\partial z_i^{L+1}}{\\partial \\theta_L}$$.\n",
    "\n",
    "Thus, you also need to compute $\\frac{\\partial z_i^{L+1}}{\\partial \\theta_{L}}$ for all $i=1,\\ldots,n_{L+1}$ where $n_{L+1}$ is the size of $z^{L+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae167a7e-7849-4a2a-ba85-8466b261ab67",
   "metadata": {},
   "source": [
    "## Solution check\n",
    "\n",
    "In order to verify your results, run the cell below, which computes the gradients using automatic differentiation in PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684364c2-3faa-47c5-976d-89a5b2c5e061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1211, 0.2423],\n",
      "        [0.2815, 0.5631],\n",
      "        [0.5272, 1.0544]])\n",
      "tensor([[0.1211],\n",
      "        [0.2815],\n",
      "        [0.5272]])\n",
      "tensor([[2.5543, 0.3196, 0.7211]])\n",
      "tensor([2.6814])\n"
     ]
    }
   ],
   "source": [
    "# Solution using PyTorch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# define here your variables. Do not forget to set requires_grad=True for W1, b1, W3, b3!\n",
    "\n",
    "W1 = torch.tensor([[0, 1], [1, -2], [-2, 1]], dtype=torch.float, requires_grad=True)\n",
    "b1 = torch.tensor([[1], [1], [-1]], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "W3 = torch.tensor([[1.0, 1, 1]], dtype=torch.float, requires_grad=True)\n",
    "b3 = torch.tensor([1.0], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "x = torch.Tensor([1, 2]).reshape(-1,1)\n",
    "y = torch.Tensor([1]).reshape(-1,1)\n",
    "\n",
    "\n",
    "W1.grad = None\n",
    "b1.grad = None\n",
    "W3.grad = None\n",
    "b3.grad = None\n",
    "\n",
    "# forward pass\n",
    "\n",
    "z1 = x\n",
    "z2 = torch.matmul(W1, z1) + b1\n",
    "z3 = torch.sigmoid(z2)\n",
    "z4 = torch.matmul(W3, z3) + b3\n",
    "z5 = torch.norm(z4-y)**2\n",
    "\n",
    "# backward pass\n",
    "z5.backward()\n",
    "\n",
    "# print results\n",
    "print(W1.grad)\n",
    "print(b1.grad)\n",
    "print(W3.grad)\n",
    "print(b3.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3872f5a4-ee7d-4ac2-be5d-b3c4753b4901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
