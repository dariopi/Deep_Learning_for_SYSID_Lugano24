{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from pytorch-ident, https://github.com/forgi86/pytorch-ident/blob/master/torchid/ss/dt/models.py\n",
    "class NeuralStateUpdate(nn.Module):\n",
    "\n",
    "    def __init__(self, n_x=2, n_u=1, n_feat=32):\n",
    "        super(NeuralStateUpdate, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_x+n_u, n_feat),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_feat, n_x),\n",
    "        )\n",
    "        \n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=1e-2)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        z = torch.cat((x, u), dim=-1)\n",
    "        dx = self.net(z)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralOutput(nn.Module):\n",
    "\n",
    "    def __init__(self, n_x, n_y, n_feat=32):\n",
    "        super(NeuralOutput, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_x, n_feat),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_feat, n_y),\n",
    "        )\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateSpaceSimulator(nn.Module):\n",
    "    def __init__(self, f_xu):\n",
    "        super().__init__()\n",
    "        self.f_xu = f_xu\n",
    "\n",
    "    def forward(self, x_0, u):\n",
    "        B, n_x = x_0.shape\n",
    "        _, T, _ = u.shape # B, T, n_u\n",
    "        x = torch.empty((B, T, n_x))\n",
    "        x_step = x_0\n",
    "\n",
    "        # manually unroll f_xu over time\n",
    "        for t in range(T): \n",
    "            x[:, t, :] = x_step\n",
    "            dx = self.f_xu(x_step, u[:, t, :])\n",
    "            x_step = x_step + dx\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 2; n_u = 1;\n",
    "f_xu = NeuralStateUpdate(n_x, n_u, n_feat=32)\n",
    "simulator = StateSpaceSimulator(f_xu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T = 32, 1024; \n",
    "batch_x0 = torch.zeros((B, n_x))\n",
    "batch_u = torch.randn((B, T, n_u)) # replace with actual training input\n",
    "batch_x_sim = simulator(batch_x0, batch_u) # B, T, n_x \n",
    "batch_x_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall, we have defined a sort of custom RNN. In fact, it behaves pretty much like a standard LSTM\n",
    "lstm = torch.nn.RNN(input_size=n_u, hidden_size=n_x, batch_first=True, num_layers=1)\n",
    "batch_h, _ = lstm(batch_u)\n",
    "batch_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_h = lstm(batch_u, hx=batch_x0[None, ...]) # hx has format (num_layers=1, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Cascaded Two-tanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_y = 1; n_x = 2; n_u = 1;\n",
    "B = 1 # just one sequence\n",
    "T = 1024\n",
    "u = torch.randn((B, T, n_u)) # replace with actual training input\n",
    "y = torch.randn((B, T, n_y)) # replace with actual training output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros((B, n_x), requires_grad=True) # this is also a training variable\n",
    "f_xu = NeuralStateUpdate(n_x, n_u, n_feat=32)\n",
    "g_x = NeuralOutput(n_x, n_y)\n",
    "simulator = StateSpaceSimulator(f_xu) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW([\n",
    "        {'params': f_xu.parameters(),    'lr': 1e-3},\n",
    "        {'params': g_x.parameters(),    'lr': 1e-3},\n",
    "        {'params': x0 , 'lr': 1e-3},\n",
    "    ], 1e-3\n",
    ")\n",
    "#opt = torch.optim.AdamW(list(f_xu.parameters()) + list(g_x.parameters()) + [x0], 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_sim = simulator(x0, u) # B, T, n_x\n",
    "y_sim = g_x(x_sim) # # B, T, n_y\n",
    "loss = torch.nn.functional.mse_loss(y, y_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
