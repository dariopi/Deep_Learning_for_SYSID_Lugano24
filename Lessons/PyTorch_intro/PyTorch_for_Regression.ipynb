{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecb60e7-0978-4623-8b79-bf5fa5356658",
   "metadata": {},
   "source": [
    "# PyTorch for static regression problems\n",
    "\n",
    "## Course on Deep Learning for System Identification\n",
    "## Authors: Gabriele Maroni\n",
    "## Lugano, April 22th, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c738db-a431-4d31-8a24-5a790e3f122a",
   "metadata": {},
   "source": [
    "In this tutorial, we will address a static regression problem using PyTorch. The focus will be on several core aspects of utilizing PyTorch for neural network development and training:\n",
    "\n",
    "1. **Defining a Custom Feed-Forward Neural Network**: We'll explore how to create a custom neural network using the `torch.nn` module. This will include setting up the network architecture with layers that are appropriate for regression tasks.\n",
    "\n",
    "2. **Writing a Custom Training Loop**: We'll go through the steps of writing a custom training loop. This loop will manage the process of feeding data to the network, calculating loss, and updating the model parameters based on the gradient.\n",
    "\n",
    "3. **Utilizing Dataset and DataLoader**: To efficiently manage data during training, especially when dealing with large datasets or when performing mini-batch gradient descent, we'll use PyTorch’s `Dataset` and `DataLoader` functionalities. These tools help in batching the data, shuffling, and loading it in parallel, which optimizes the training process.\n",
    "\n",
    "By the end of this notebook, you will have a clear understanding of how to implement these components in PyTorch, which are fundamental to building and training neural network models effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17df771-55e6-4c8a-a427-2b19f9bdb910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "from configs import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "rcParams.update(fig_params)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa033f-39d9-4c31-9deb-61f8bccb7ebb",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194abe83-04e8-49d6-8e8d-44ab42b2732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed value for reproducibility\n",
    "SEED=42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f56995-9081-4f83-8b71-cad8b8d0bd2c",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048d5ec-6196-42b8-b7d2-e99eedb2d45c",
   "metadata": {},
   "source": [
    "The dataset that will be used in this notebook is simulated according to the following data generation mechanism [1]:\n",
    "\n",
    "\\begin{align}\n",
    "x_i &\\sim \\mathcal{N}(0,9), \\quad i=1,...,20, \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(0,\\sigma_{\\epsilon}) \\\\\n",
    "y &= x_1 + \\text{sin}(x2) + \\text{log}(\\left|x3\\right|) + x_4^2 + x_5x_6 + \\mathbb{I}(x_7x_8x_9<0) + \\mathbb{I}(x_{10}>0) \\\\\n",
    "&+ x_{11}\\mathbb{I}(x_{11}>0) + \\sqrt{\\left|x_{12}\\right|} + \\text{cos}(x_{13}) + 2x_{14} + \\left|x_{15}\\right| + \\mathbb{I}(x_{16}<-1) \\\\\n",
    "&+ x_{17}\\mathbb{I}(x_{17}<-1) - 2x_{18} - x_{19}x_{20} + \\epsilon\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e80994-285b-41e5-867b-0e77ec1e02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "df, features, target = generate_regression_dataset(n_samples=10000,\n",
    "                                                   snr_db=20, # signal to noise ratio in dB\n",
    "                                                   random_seed=SEED)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889fa17-648f-4014-a6b9-48ae9a619d30",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff28e32-cf75-4d70-9a00-fdb9cf70cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=SEED)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab35114-428f-45b2-bdc6-b0a9c170259b",
   "metadata": {},
   "source": [
    "## Modeling: linear baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c5e8f-331d-4b41-8d39-51a94f6b83ad",
   "metadata": {},
   "source": [
    "Starting the modeling phase with a baseline model such as a linear regression can be beneficial. It offers a simple, understandable reference point for evaluating more complex models. \n",
    "Additionally, it helps quickly identify if the problem can be reasonably addressed with basic approaches before moving to more sophisticated techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f752b2-af18-4bba-a9ae-e25683b6911e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cace453-5164-4e66-b4d4-40f1352a934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(train[features], train[target]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7038ea-9fe4-4b40-876b-76f702d2f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve learned coefficients\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed5714-5bbe-462e-972a-e2e313e92158",
   "metadata": {},
   "source": [
    "### Predicting and computing evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c52d23-3536-4110-ab44-f581d1521f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train prediction\n",
    "y_pred_train = model.predict(train[features])\n",
    "# Test prediction\n",
    "y_pred_test = model.predict(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e732e5-63d2-4461-bf57-6e6fd15f01c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics on train\n",
    "calculate_metrics(train[target], y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a0706-b30f-4f36-bc20-d730c8968e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute evaluation metrics on test\n",
    "calculate_metrics(test[target], y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a22d0-4fe3-4d03-9ad2-a309f64b0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis of the results\n",
    "plot_diagnostics(train[target], y_pred_train, test[target], y_pred_test, model_name='Linear regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dba602-edae-4f3e-b798-c905a50c5445",
   "metadata": {},
   "source": [
    "From analyzing the diagnostic results, it is evident that the $R^2$ scores for both the training and test datasets are closely aligned, with the distributions of the residuals showing significant overlap. This consistency suggests that there is no overfitting in the training dataset. However, the similar but low performance metrics across both datasets also imply that the linear model is not adequately capturing the complex relationships between the features and the target variable. This indicates underfitting within the training datasets. Consequently, to better model these complexities and improve predictive accuracy, it is necessary to consider more sophisticated modeling approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d6e17-4093-4d5d-a712-6b58921e20ea",
   "metadata": {},
   "source": [
    "## Modeling: Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61872699-32dc-414d-b9af-b22f5613afc3",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c7aa0-1964-4ce5-9eb7-5ce14e0ab9af",
   "metadata": {},
   "source": [
    "Data normalization is a critical preprocessing step in deep learning that brings multiple advantages to model training. By ensuring all input features share a consistent scale, it prevents any single feature from dominating others, promoting stable convergence and more uniform gradient updates. This uniformity not only enhances the training efficiency and speeds up convergence but also reduces numerical instabilities like overflow or underflow, often referred to as “exploding gradients”. Additionally, normalized data smoothens the error surface, facilitating a more direct path for optimization algorithms and improving model generalization to handle new, unseen data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f84d5-725a-4459-bfab-3a9bd8b01d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "x_scaler = StandardScaler()\n",
    "x_train = x_scaler.fit_transform(train[features].values)\n",
    "x_test = x_scaler.transform(test[features].values)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train = y_scaler.fit_transform(train[target].values.reshape(-1,1))\n",
    "y_test = y_scaler.transform(test[target].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10bff2-bb40-4c68-b4ad-1c63b7e61dd8",
   "metadata": {},
   "source": [
    "Converting data from numpy arrays or pandas DataFrames to PyTorch tensors is essential for training because tensors support automatic differentiation, which enables the computation of gradients during backpropagation. This feature is crucial for optimizing model parameters effectively throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af675df-4744-4c9f-a89c-e6d32a8a8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to PyTorch tensors\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692c9a1-fbcf-4869-a3f8-89397be9f8f4",
   "metadata": {},
   "source": [
    "### Defining Custom Feed-Forward Neural Networks in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abd798-0519-4760-8af8-d46c5bc69a61",
   "metadata": {},
   "source": [
    "The following code blocks define three different PyTorch neural network models using the `torch.nn.Module` class. Each model is a type of feedforward neural network with slight variations in their architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426660b7-d137-4efc-b77c-5fae411e70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForwardNN1, self).__init__()\n",
    "        # First layer\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second layer\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer with ReLU activation\n",
    "        x = self.relu(self.linear1(x))\n",
    "        # Output layer, no activation function for regression\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f3664a-5947-44cf-8978-336c99091630",
   "metadata": {},
   "source": [
    "#### FeedForwardNN1\n",
    "- **Purpose**: A simple feedforward neural network with one hidden layer.\n",
    "- **Initialization** (`__init__`):\n",
    "  - **`input_size`**: Dimensionality of the input features.\n",
    "  - **`hidden_size`**: Number of neurons in the hidden layer.\n",
    "  - **`output_size`**: Number of output neurons (e.g., for regression or classification).\n",
    "  - **Layers**:\n",
    "    - `self.linear1`: First fully connected layer mapping from input to hidden layer.\n",
    "    - `self.relu`: ReLU activation function to introduce non-linearity.\n",
    "    - `self.linear2`: Second fully connected layer mapping from hidden to output layer.\n",
    "- **Forward Pass** (`forward`):\n",
    "  - Data `x` is passed through the first layer and ReLU activation.\n",
    "  - The output of the ReLU is then passed to the second layer. Note that there's no activation function after the second layer, making this suitable for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989141f-0ee8-48e4-b5d0-89f940056ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FeedForwardNN1(input_size=x_train.shape[1],\n",
    "                        hidden_size=10,\n",
    "                        output_size=y_train.shape[1])\n",
    "summary(model1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538a278-94b2-4fb2-88e1-251cf782cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(FeedForwardNN2, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        # Second hidden layer\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        # Output layer\n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the first hidden layer and apply ReLU\n",
    "        x = self.relu(self.linear1(x))\n",
    "        # Pass through the second hidden layer and apply ReLU\n",
    "        x = self.relu(self.linear2(x))\n",
    "        # Pass through the output layer, no activation function for regression\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03229b78-8525-4814-be35-61f727a68a3c",
   "metadata": {},
   "source": [
    "#### FeedForwardNN2\n",
    "- **Purpose**: A deeper feedforward neural network with two hidden layers.\n",
    "- **Initialization** (`__init__`):\n",
    "  - **`input_size`**: Dimensionality of input features.\n",
    "  - **`hidden_size1` and `hidden_size2`**: The number of neurons in the first and second hidden layers, respectively.\n",
    "  - **`output_size`**: Number of output neurons.\n",
    "  - **Layers**:\n",
    "    - `self.linear1`, `self.linear2`, and `self.linear3`: Three linear (fully connected) layers, where each successive layer connects one hidden layer to another or to the output.\n",
    "    - `self.relu`: ReLU activation function to introduce non-linearity.\n",
    "- **Forward Pass** (`forward`):\n",
    "  - Sequentially passes data `x` through the two hidden layers with ReLU activations and then through the output layer without activation, suitable for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae7ff1-03b3-430f-af1b-e87cbe3a0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FeedForwardNN2(input_size=x_train.shape[1],\n",
    "                        hidden_size1=10,\n",
    "                        hidden_size2=10,\n",
    "                        output_size=y_train.shape[1])\n",
    "summary(model2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e29f9-785f-42ff-a1e4-c265af9ee841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        layers = []\n",
    "        # Adding the first hidden layer that connects the input layer to the first hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Creating subsequent hidden layers\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Adding the output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        # Wrap all layers in ModuleList so they are properly registered.\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069497d-367e-47ed-8c70-d41461c08a25",
   "metadata": {},
   "source": [
    "#### FeedForwardNN\n",
    "- **Purpose**: A modular and scalable model with a variable number of hidden layers.\n",
    "- **Initialization** (`__init__`):\n",
    "  - **`input_size`**: Dimensionality of input features.\n",
    "  - **`hidden_sizes`**: A list specifying the number of neurons in each hidden layer.\n",
    "  - **`output_size`**: Number of output neurons.\n",
    "  - **Layers**:\n",
    "    - Dynamically creates layers based on the `hidden_sizes` list.\n",
    "    - Uses a combination of `nn.Linear` and `nn.ReLU` for each hidden layer.\n",
    "    - `self.layers`: `nn.ModuleList` wraps the layers ensuring proper parameter registration and handling within PyTorch’s optimization framework.\n",
    "- **Forward Pass** (`forward`):\n",
    "  - Iteratively processes data `x` through each layer (both `nn.Linear` and `nn.ReLU`) in the `self.layers` list, allowing for flexibility in handling different numbers of layers and complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c97ad-f372-415b-bb0d-079189a3ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardNN(input_size=x_train.shape[1],\n",
    "                      hidden_sizes=[10,10],\n",
    "                      output_size=y_train.shape[1])\n",
    "summary(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9277e-9023-4e95-b307-2e799682d6fb",
   "metadata": {},
   "source": [
    "### Training set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b9288-030a-49da-8e38-a0f9eb5efc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "# Instantiate the model\n",
    "model = FeedForwardNN(input_size=x_train.shape[1],\n",
    "                      hidden_sizes=[20,20],\n",
    "                      output_size=y_train.shape[1])\n",
    "print(f\"Total number of trainable model parameters: {get_model_num_params(model)}\")\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Calculate loss value with untrained model to use as reference\n",
    "y_pred_train = model(x_train)\n",
    "loss0 = loss_function(y_pred_train, y_train).item()\n",
    "print(f\"Initial loss: {loss0:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b5393-c3b6-42cd-8dec-90bae72efca5",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e57276-f36d-45ad-83a7-a7483f706383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the number of epoch, which determines the number of training iterations. Each epoch represents a complete pass over the entire training dataset.\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred_train = model(x_train) # Compute predicted y by passing x to the model\n",
    "    \n",
    "    loss = loss_function(y_pred_train, y_train) # Compute the loss\n",
    "    \n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad() # Clear the gradients before backward pass to not accumulate gradients over multiple iterations.\n",
    "                          # You can thus call optimizer.zero_grad() everywhere in the loop but not between the loss.backward() and optimizer.step() operation.\n",
    "    loss.backward() # Backpropagation: computes the gradient of the loss with respect to the model parameters.\n",
    "    optimizer.step() # Update parameters: adjusts the model parameters based on the computed gradients.\n",
    "    \n",
    "    # Logging the loss\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch:03d}/{epochs:03d}\"\n",
    "              f\" | Train Loss: {loss:.5f}\")\n",
    "\n",
    "# Optionally, evaluate the model performance\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(x_train)\n",
    "    final_loss = loss_function(y_pred_train, y_train)\n",
    "    print(f'Final Loss: {final_loss.item():.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590cbbf-85a5-4ee8-a9de-27705d1f0fb8",
   "metadata": {},
   "source": [
    "### Predicting and computing evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2f204-b8ae-4f3f-9cb5-cc76ba890f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIRST OPTION:\n",
    "# Train prediction\n",
    "y_pred_train = model(x_train)\n",
    "# Test prediction\n",
    "y_pred_test = model(x_test)\n",
    "\n",
    "# Invert scaling operation\n",
    "y_pred_train = y_scaler.inverse_transform(y_pred_train.detach().numpy())\n",
    "y_pred_test = y_scaler.inverse_transform(y_pred_test.detach().numpy())\n",
    "\n",
    "### SECOND OPTION:\n",
    "# with torch.no_grad():\n",
    "#     # Train prediction\n",
    "#     y_pred_train = model(x_train)\n",
    "#     # Test prediction\n",
    "#     y_pred_test = model(x_test)\n",
    "    \n",
    "#     # Invert scaling operation\n",
    "#     y_pred_train = y_scaler.inverse_transform(y_pred_train.numpy())\n",
    "#     y_pred_test = y_scaler.inverse_transform(y_pred_test.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5727365-d6ed-47d7-a1f1-34c7d4d6d159",
   "metadata": {},
   "source": [
    "#### First Option: Without `torch.no_grad()`\n",
    "- **Gradient Tracking**: By default, PyTorch tracks operations on tensors for gradient computation. Even if you are just making predictions, without `with torch.no_grad()`, the tensors produced (like `y_pred_train` and `y_pred_test`) will have their computational graph retained, and PyTorch will keep track of any operations performed on them. This uses more memory and computation.\n",
    "- **Detaching and Converting to NumPy**: You detach the tensors from the computational graph using `.detach()` before converting them to NumPy arrays. This prevents PyTorch from tracking further operations, which is necessary to move the data to a library like NumPy that does not understand PyTorch gradients.\n",
    "\n",
    "#### Second Option: Using `with torch.no_grad()`\n",
    "- **Disabling Gradient Computation**: The `with torch.no_grad():` block explicitly stops PyTorch from tracking operations on tensors. This means that during inference, the tensors do not track gradients, reducing memory usage and computational overhead.\n",
    "- **Direct Conversion to NumPy**: Inside the `with torch.no_grad():` block, since gradients are not being tracked, there is no need to call `.detach()` on the tensors before converting them to NumPy arrays. You can directly convert the tensors to NumPy, which simplifies the code and maintains lower memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe5035-87a1-41f1-b952-d8c01c0b1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(train[target], y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4f75be-8328-4e07-89df-72cd16699940",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(test[target], y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09b5cf-dcce-4d83-a3d2-cc6a9e232fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis of the results\n",
    "plot_diagnostics(train[target], y_pred_train, test[target], y_pred_test, model_name='FeedForwardNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a133f72d-7dd7-41a3-bdca-6ef6b7bbfdd1",
   "metadata": {},
   "source": [
    "From analyzing the results, it is evident that the feedforward neural network, in contrast to the linear model, successfully captures the strong non-linearities inherent in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af8e7f0-4903-4ea4-a01a-64e0ced350b8",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52945f0d-2d2b-4202-a7d0-e9ac709f316e",
   "metadata": {},
   "source": [
    "Utilizing PyTorch’s Dataset and DataLoader functionalities is useful for managing data effectively during the training of machine learning models, particularly when dealing with large datasets or when implementing mini-batch gradient descent. The `Dataset` class provides a structured way to encapsulate any type of data, be it images, text, or tabular data into a format that can be easily manipulated and passed through machine learning models. This encapsulation allows for the isolation of data preprocessing steps from the model training logic, making the code more modular and easier to maintain.\n",
    "\n",
    "The `DataLoader`, on the other hand, enhances the training process by automating the retrieval of data samples from a `Dataset` in batches. This is particularly important for training on large datasets that do not fit into memory all at once. By training the model on smaller subsets or \"mini-batches,\" mini-batch gradient descent allows for both memory efficiency and faster convergence by updating model parameters iteratively after each batch rather than waiting to process the entire dataset.\n",
    "\n",
    "Moreover, `DataLoader` supports shuffling of the data at the beginning of each training epoch, which prevents the model from learning patterns based on the order of the data, thus helping to avoid overfitting and ensuring better model generalization. Parallel data loading is another significant feature provided by `DataLoader`, leveraging multiple worker processes to load data simultaneously. This parallelism significantly reduces the time the model spends waiting for data to be ready, thereby optimizing the overall training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2810ed-e206-429a-970d-30c86ce4af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f929524-c850-476d-ad66-5f0d0481132c",
   "metadata": {},
   "source": [
    "#### Custom Dataset Class `RegressionDataset`\n",
    "\n",
    "- **Class Definition**: `RegressionDataset` inherits from PyTorch's `Dataset` class, which is a part of the `torch.utils.data` module. This inheritance makes `RegressionDataset` compatible with other PyTorch utilities like `DataLoader`.\n",
    "\n",
    "- **Constructor `__init__`**:\n",
    "  - The constructor takes two arguments: `x` and `y`. These are typically numpy arrays, PyTorch tensors, or lists containing the input features (`x`) and the target outputs (`y`) respectively.\n",
    "  - `self.x` and `self.y` are assigned to store these inputs and outputs within the dataset object.\n",
    "\n",
    "- **`__len__` Method**:\n",
    "  - This method returns the number of examples in the dataset by returning the length of `self.x`. This is essential for the `DataLoader` to determine the size of the dataset.\n",
    "\n",
    "- **`__getitem__` Method**:\n",
    "  - Takes an index `idx` and returns a tuple containing the input features and target output for the corresponding example. This method is called by the `DataLoader` to fetch batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c0ee9-6c5e-4889-b2ab-033a15ce6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Datasets\n",
    "train_dataset = RegressionDataset(x_train, y_train)\n",
    "test_dataset = RegressionDataset(x_test, y_test)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361c3a6-2651-4301-94e4-5e944b641321",
   "metadata": {},
   "source": [
    "#### Data Loaders\n",
    "\n",
    "- **`batch_size` Variable**: \n",
    "  - A variable `batch_size` is set to 64, indicating the number of samples included in each batch when the `DataLoader` retrieves data.\n",
    "\n",
    "- **DataLoader Instances**:\n",
    "  - **Training DataLoader (`train_loader`)**: Configured to load data from `train_dataset`. It uses the specified `batch_size` and shuffles the data (`shuffle=True`) to ensure that each epoch receives the data in a different order, which helps in improving model generalization. `drop_last=True` is used to discard the last batch in each epoch if it's smaller than the specified `batch_size`, ensuring that all batches have the same size.\n",
    "  - **Testing DataLoader (`test_loader`)**: Configured similarly but with `shuffle=False`, as shuffling is generally not required during testing or validation because it does not affect the outcome but can make results non-reproducible if the test data is randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a65a9-ff1e-4745-b149-c82eb3e0cf11",
   "metadata": {},
   "source": [
    "Choosing the batch size for training a neural network is a critical decision that can significantly influence the training dynamics, performance, and computational efficiency. There is no one-size-fits-all answer; instead, the optimal batch size often depends on several factors, including the specific problem, the architecture of the model, and the hardware available. Here are some key considerations and strategies to help determine the best batch size for your needs:\n",
    "\n",
    "#### 1. **Memory Constraints**\n",
    "- **Hardware Limits**: The primary constraint is often the amount of GPU or CPU memory available. Larger batch sizes require more memory since more data must be processed simultaneously. You need to choose a batch size that your hardware can handle without running out of memory.\n",
    "\n",
    "#### 2. **Training Stability and Performance**\n",
    "- **Generalization vs. Speed**: Larger batch sizes can lead to faster epoch times as you're processing more data simultaneously, leveraging efficient parallel hardware computations. However, larger batches might reduce the model's ability to generalize well to unseen data. This happens because large batch sizes provide a more accurate estimate of the gradient, which might lead to convergence to sharp minima that do not generalize well.\n",
    "- **Small Batch Sizes**: Training with smaller batch sizes has been observed to provide a regularizing effect and better generalization. However, very small batches can lead to a noisy gradient estimate, which might make the training process unstable and increase the time to converge.\n",
    "\n",
    "#### 3. **Empirical Heuristics**\n",
    "- **Rule of Thumb**: A common practice is to try a range of batch sizes, such as 32, 64, 128, 256, and so on. Monitor the training performance for each size and select the one that offers the best compromise between training speed and model accuracy.\n",
    "- **Powers of Two**: Batch sizes that are powers of two (e.g., 32, 64, 128) are often recommended due to the way memory is structured in most computing hardware, potentially leading to more efficient processing.\n",
    "\n",
    "#### 4. **Learning Dynamics**\n",
    "- **Batch Size and Learning Rate**: The batch size can affect the optimal learning rate. Larger batches typically require a higher learning rate since the gradient estimate is less noisy. Techniques like learning rate scaling can be helpful; for instance, linear scaling rule suggests scaling the learning rate in proportion to the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505cb799-dda7-424c-aa6a-fce78ed439b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_train_batch, y_train_batch in train_loader:\n",
    "    print(x_train_batch.shape, y_train_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3c1e25-f35f-42bf-9515-2190dc06781a",
   "metadata": {},
   "source": [
    "### Training set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14336950-452e-4a76-ada4-2c10bf340a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)\n",
    "\n",
    "# Instantiate the model\n",
    "model = FeedForwardNN(input_size=x_train.shape[1],\n",
    "                      hidden_sizes=[20,20],\n",
    "                      output_size=y_train.shape[1])\n",
    "print(f\"Total number of trainable model parameters: {get_model_num_params(model)}\")\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Calculate loss value with untrained model to use as reference\n",
    "y_pred_train = model(x_train)\n",
    "loss0 = loss_function(y_pred_train, y_train).item()\n",
    "print(f\"Initial loss: {loss0:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a776b05-e7c1-4236-9814-0398ec347691",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fa166-4822-47c0-a289-e772bd713a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    loss = 0.0\n",
    "    for x_train_batch, y_train_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_train_batch = model(x_train_batch)\n",
    "        loss_batch = loss_function(y_pred_train_batch, y_train_batch)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss_batch.item() * x_train_batch.size(0)\n",
    "\n",
    "    # Average loss\n",
    "    loss /= train.shape[0]\n",
    "\n",
    "    # Logging\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch:03d}/{epochs:03d}\"\n",
    "              f\" | Train Loss: {loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efc8f6-4419-44cf-a168-004580f2e3df",
   "metadata": {},
   "source": [
    "### Predicting and computing evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68563dac-e0d4-4350-9668-d3775e76bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIRST OPTION:\n",
    "with torch.no_grad():\n",
    "    # Train prediction\n",
    "    y_pred_train = model(x_train)\n",
    "    # Test prediction\n",
    "    y_pred_test = model(x_test)\n",
    "    \n",
    "    # Invert scaling operation\n",
    "    y_pred_train = y_scaler.inverse_transform(y_pred_train.numpy())\n",
    "    y_pred_test = y_scaler.inverse_transform(y_pred_test.numpy())\n",
    "\n",
    "# ### SECOND OPTION:\n",
    "# y_pred_test = np.empty(0)\n",
    "# with torch.no_grad(): \n",
    "#     for x_test_batch, _ in test_loader:\n",
    "#         y_pred_test_batch = model(x_test_batch)\n",
    "#         y_pred_test = np.append(y_pred_test, y_pred_test_batch.numpy())\n",
    "# y_pred_test = y_scaler.inverse_transform(y_pred_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0520b-148f-4bc9-92eb-a2dca409b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(train[target], y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655e8c9-3114-434c-a67e-aace1ebfe4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(test[target], y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d8df5b-965f-4dd9-9056-c94308787baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis of the results\n",
    "plot_diagnostics(train[target], y_pred_train, test[target], y_pred_test, model_name='FeedForwardNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb84ad5-05eb-4661-9a23-309ea3d1783c",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71431b85-c2d1-4188-9826-08e5517dccfd",
   "metadata": {},
   "source": [
    "Familiarize with the code, experiment with different:\n",
    "- number of hidden layers\n",
    "- number of neurons\n",
    "- optimizers\n",
    "- learning rates\n",
    "- epochs number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1263a0-58b1-407f-936e-58ed9b69636a",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c587d-ea92-4378-b353-def75475afee",
   "metadata": {},
   "source": [
    "[1] Kuhn, M. and Johnson, K., 2019. Feature engineering and selection: A practical approach for predictive models. Chapman and Hall/CRC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
